{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "papers=pd.read_csv(\"C:\\\\Users\\\\hp\\\\Downloads\\\\Corona_Preprocessed.csv\")\n",
    "#papers.to_csv('/content/drive/My Drive/Data/Twitter/Sports.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:129: DeprecationWarning: invalid escape sequence \\)\n",
      "<>:129: DeprecationWarning: invalid escape sequence \\)\n",
      "<>:129: DeprecationWarning: invalid escape sequence \\)\n",
      "<ipython-input-3-2d0227b92829>:129: DeprecationWarning: invalid escape sequence \\)\n",
      "  text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import unidecode\n",
    "import inflect\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#Download Stop Words If Necessary\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('words')\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "#Global Variables\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "#Slang Map For Words With Slang\n",
    "with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\slang.txt\") as file:\n",
    "        slang_map = dict(map(str.strip, line.partition('\\t')[::2]) for line in file if line.strip())\n",
    "\n",
    "#Inflection Engine For Number To Word Conversion\n",
    "inflectEngine = inflect.engine()\n",
    "\n",
    "#Custom Functions\n",
    "\n",
    "#Remove \"RT\" tag\n",
    "def removeRT(text):\n",
    "    text = text.replace(\"RT\", \"\", 1)\n",
    "    return text\n",
    "    \n",
    "#Removing HTML Tags\n",
    "def stripHtmlTags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "#Remove URL\n",
    "def removeURL(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "#Remove Non English Words\n",
    "def removeNonEnglish(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
    "    return text\n",
    "\n",
    "#Contraction Expansion\n",
    "def expandContractions(text, contraction_mapping = CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "#Removing Accented Characters\n",
    "def removeAccentedChars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "#Remove User Mentions\n",
    "def removeUserMentions(text):\n",
    "    text = re.sub(r\"(?:\\@)\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "#Convert Text To Lower Case\n",
    "def lowerCaseConversion(text):\n",
    "    text = \" \".join(x.lower() for x in str(text).split())\n",
    "    return text\n",
    "\n",
    "#Remove special characters\n",
    "def specialCharsRemoval(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#Remove Emoticons\n",
    "def removeEmoticons(text):\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "#Remove Stop Words\n",
    "def stopWordsRemoval(text):\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
    "                 'would','really','like','great','service','came','got'])\n",
    "    text = \" \".join([x for x in text.split() if x not in stop])\n",
    "    return text\n",
    "\n",
    "#Convert Numbers To Words\n",
    "def numberToWords(text):  \n",
    "    temp_str = text.split() \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        if word.isdigit(): \n",
    "            temp = inflectEngine.number_to_words(word) \n",
    "            new_string.append(temp)\n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    text = ' '.join(new_string) \n",
    "    return text\n",
    "\n",
    "#Remove Numbers From Text\n",
    "def removeNumbers(text):\n",
    "    for word in text.split():\n",
    "        if word.isnumeric():\n",
    "            text = text.replace(word, '')\n",
    "    return text\n",
    "\n",
    "#Remove Adverbs\n",
    "def removeAdverbs(text):\n",
    "    adverbs = ['accidentally', 'always', 'angrily', 'anxiously', 'awkwardly', 'badly', 'blindly', 'boastfully', 'boldly', 'bravely', 'brightly', 'cheerfully', 'coyly', 'crazily', 'defiantly', 'deftly', 'deliberately', 'devotedly', 'doubtfully', 'dramatically', 'dutifully', 'eagerly', 'elegantly', 'enormously', 'evenly', 'eventually', 'exactly', 'faithfully', 'finally', 'foolishly', 'fortunately', 'frequently', 'gleefully', 'gracefully', 'happily', 'hastily', 'honestly', 'hopelessly', 'hourly', 'hungrily', 'innocently', 'inquisitively', 'irritably', 'jealously', 'justly', 'kindly', 'lazily', 'loosely', 'madly', 'merrily', 'mortally', 'mysteriously', 'nervously', 'never', 'obediently', 'obnoxiously', 'occasionally', 'often', 'only', 'perfectly', 'politely', 'poorly', 'powerfully', 'promptly', 'quickly', 'rapidly', 'rarely', 'regularly', 'rudely', 'safely', 'seldom', 'selfishly', 'seriously', 'shakily', 'sharply', 'silently', 'slowly', 'solemnly', 'sometimes', 'speedily', 'sternly', 'technically', 'tediously', 'unexpectedly', 'usually', 'victoriously', 'vivaciously', 'warmly', 'wearily', 'weekly', 'wildly', 'yearly', 'not', 'also', 'very', 'often', 'however', 'too', 'usually', 'really', 'early', 'never', 'always', 'sometimes', 'together', 'likely', 'simply', 'generally', 'instead', 'actually', 'again', 'rather', 'almost', 'especially', 'ever', 'quickly', 'probably', 'already', 'below', 'directly', 'therefore', 'else', 'thus', 'easily', 'eventually', 'exactly', 'certainly', 'normally', 'currently', 'extremely', 'finally', 'constantly', 'properly', 'soon', 'specifically', 'ahead', 'daily', 'highly', 'immediately', 'relatively', 'slowly', 'fairly', 'primarily', 'completely', 'ultimately', 'widely', 'recently', 'seriously', 'frequently', 'fully', 'mostly', 'naturally', 'nearly', 'occasionally', 'carefully', 'clearly', 'essentially', 'possibly', 'slightly', 'somewhat', 'equally', 'greatly', 'necessarily', 'personally', 'rarely', 'regularly', 'similarly', 'basically', 'closely', 'effectively', 'initially', 'literally', 'mainly', 'merely', 'gently', 'hopefully', 'originally', 'roughly', 'significantly', 'totally', 'twice', 'elsewhere', 'everywhere', 'obviously', 'perfectly', 'physically', 'successfully', 'suddenly', 'truly', 'virtually', 'altogether', 'anyway', 'automatically', 'deeply', 'definitely', 'deliberately', 'hardly', 'readily', 'terribly', 'unfortunately', 'forth', 'briefly', 'moreover', 'strongly', 'honestly', 'previously', 'as', 'there', 'when', 'how', 'so', 'up', 'out', 'no', 'only', 'well', 'then', 'first', 'where', 'why', 'now', 'around', 'once', 'down', 'off', 'here', 'tonight', 'away', 'today', 'far', 'quite', 'later', 'above', 'yet', 'maybe', 'otherwise', 'near', 'forward', 'somewhere', 'anywhere', 'please', 'forever', 'somehow', 'absolutely', 'abroad', 'yeah', 'nowhere', 'tomorrow', 'yesterday', 'the', 'to', 'in', 'on', 'by', 'more', 'about', 'such', 'through', 'new', 'just', 'any', 'each', 'much', 'before', 'between', 'free', 'right', 'best', 'since', 'both', 'sure', 'without', 'back', 'better', 'enough', 'lot', 'small', 'though', 'less', 'little', 'under', 'next', 'hard', 'real', 'left', 'least', 'short', 'last', 'within', 'along', 'lower', 'TRUE', 'bad', 'across', 'clear', 'easy', 'full', 'close', 'late', 'proper', 'fast', 'wide', 'item', 'wrong', 'ago', 'behind', 'quick', 'straight', 'direct', 'extra', 'morning', 'pretty', 'overall', 'alone', 'bright', 'flat', 'whatever', 'slow', 'clean', 'fresh', 'whenever', 'cheap', 'thin', 'cool', 'fair', 'fine', 'smooth', 'FALSE', 'thick', 'collect', 'nearby', 'wild', 'apart', 'none', 'strange', 'tourist', 'aside', 'loud', 'super', 'tight', 'gross', 'ill', 'downtown', 'honest', 'ok', 'pray', 'weekly']\n",
    "    text = \" \".join([x for x in text.split() if x not in adverbs])\n",
    "    return text\n",
    "\n",
    "#Remove Slang Words\n",
    "def removeSlangWords(text):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in slang_map.keys():\n",
    "            text = text.replace(word, slang_map[word])\n",
    "    return text\n",
    "\n",
    "#Remove Whitespace From Text \n",
    "def removeWhitespace(text): \n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "#Main Data Preprocessing Method\n",
    "def dataPreprocessing(corpus, RTremoval = True, userMentionRemoval = True, toLowerCase = True, stripHTML = True, URLRemoval = True, \n",
    "                      expandContraction = True, slangWordsRemoval = True, stripAccentedChars = True, NonEnglishRemoval = True, \n",
    "                      removeSplChars = True, removeStopWords = True, adverbsRemoval = True, numbersRemoval = True, numToWords = False, \n",
    "                      whiteSpaceRemoval = True):\n",
    "    for i in range(len(corpus)):\n",
    "        clear_output(wait=True)\n",
    "        print((\"%d of %d tweets processed...\")%(i+1, len(corpus)))\n",
    "        if RTremoval:\n",
    "            corpus[i] = removeRT(corpus[i])\n",
    "        if userMentionRemoval:\n",
    "            corpus[i] = removeUserMentions(corpus[i])\n",
    "        if toLowerCase:\n",
    "            corpus[i] = lowerCaseConversion(corpus[i])\n",
    "        if stripHTML:\n",
    "            corpus[i] = stripHtmlTags(corpus[i])\n",
    "        if URLRemoval:\n",
    "            corpus[i] = removeURL(corpus[i])\n",
    "        if expandContraction:\n",
    "            corpus[i] = expandContractions(corpus[i])\n",
    "        if slangWordsRemoval:\n",
    "            corpus[i] = removeSlangWords(corpus[i])\n",
    "        if stripAccentedChars:\n",
    "            corpus[i] = removeAccentedChars(corpus[i])\n",
    "        if NonEnglishRemoval:\n",
    "            corpus[i] = removeNonEnglish(corpus[i])\n",
    "        if removeSplChars:\n",
    "            corpus[i] = specialCharsRemoval(corpus[i])\n",
    "        if removeStopWords:\n",
    "            corpus[i] = stopWordsRemoval(corpus[i])\n",
    "        if adverbsRemoval:\n",
    "            corpus[i] = removeAdverbs(corpus[i])\n",
    "        if numbersRemoval:\n",
    "            corpus[i] = removeNumbers(corpus[i])\n",
    "        if numToWords:\n",
    "            corpus[i] = numberToWords(corpus[i])\n",
    "        if whiteSpaceRemoval:\n",
    "            corpus[i] = removeWhitespace(corpus[i])\n",
    "    return(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "papers['time'] = pd.to_datetime(papers['time']) \n",
    "papers[\"time\"] = pd.to_datetime(papers[\"time\"]).dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>corona wsh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>aynen.testereyle devam edip,suriye ve corona v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>bra beslut av staffanstorp/ kommunalrådet chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>dua to protect you from the corona virus and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>¿se acuerdan de las vacas locas? ¿se acuerdan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048570</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>just wondering! did india ask the religious fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048571</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>if corona has your gym closed then try my home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048572</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>mano, as estradas do norte da itália vazias po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048573</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>holy shit i hate every person in this video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048574</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>pak gatot, bahkan vatikan juga tutup gereja2 d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1038896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time                                              tweet\n",
       "0        01-03-2020                                         corona wsh\n",
       "1        01-03-2020  aynen.testereyle devam edip,suriye ve corona v...\n",
       "2        01-03-2020  bra beslut av staffanstorp/ kommunalrådet chri...\n",
       "3        01-03-2020  dua to protect you from the corona virus and o...\n",
       "4        01-03-2020  ¿se acuerdan de las vacas locas? ¿se acuerdan ...\n",
       "...             ...                                                ...\n",
       "1048570  19-03-2020  just wondering! did india ask the religious fa...\n",
       "1048571  19-03-2020  if corona has your gym closed then try my home...\n",
       "1048572  19-03-2020  mano, as estradas do norte da itália vazias po...\n",
       "1048573  19-03-2020        holy shit i hate every person in this video\n",
       "1048574  19-03-2020  pak gatot, bahkan vatikan juga tutup gereja2 d...\n",
       "\n",
       "[1038896 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import numpy as np\n",
    " papers[\"time\"] = np.where(papers[\"time\"]=='NaT',np.nan, papers[\"time\"] )\n",
    " papers.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>corona wsh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>aynen.testereyle devam edip,suriye ve corona v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>bra beslut av staffanstorp/ kommunalrådet chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>dua to protect you from the corona virus and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>01-03-2020</td>\n",
       "      <td>¿se acuerdan de las vacas locas? ¿se acuerdan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time                                              tweet\n",
       "0  01-03-2020                                         corona wsh\n",
       "1  01-03-2020  aynen.testereyle devam edip,suriye ve corona v...\n",
       "2  01-03-2020  bra beslut av staffanstorp/ kommunalrådet chri...\n",
       "3  01-03-2020  dua to protect you from the corona virus and o...\n",
       "4  01-03-2020  ¿se acuerdan de las vacas locas? ¿se acuerdan ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['time'].dropna(inplace=True)\n",
    "abcd=papers['time'].unique()\n",
    "papers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01-03-2020', '03-03-2020', nan, '04-03-2020', '05-03-2020',\n",
       "       '07-03-2020', '08-03-2020', '09-03-2020', '10-03-2020',\n",
       "       '11-03-2020', '13-03-2020', '14-03-2020', '15-03-2020',\n",
       "       '16-03-2020', '17-03-2020', '19-03-2020'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['tweet'] = papers['tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "for i in abcd[-11:]:\n",
    "    print(\"For \"+ i)\n",
    "    il=0\n",
    "    dflist=[]\n",
    "    for j in range(1,len(papers['tweet'])):\n",
    "        if papers['time'][j]==i:\n",
    "            dflist.append(papers['tweet'][j])\n",
    "    if len(dflist)>=20000: \n",
    "        print(\"Preprocessing starts.. if condt\")\n",
    "        dflist=dataPreprocessing(dflist[:20000])\n",
    "        print (len(dflist))\n",
    "    else:\n",
    "        print(\"Preprocessing starts.. else condt\")\n",
    "        dflist=dataPreprocessing(dflist)        \n",
    "        print (len(dflist))\n",
    "    \n",
    "    vectorizer = CountVectorizer(      \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                                         # remove stop words\n",
    "                             lowercase=False # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "    data_vectorized = vectorizer.fit_transform(dflist)\n",
    "    # Materialize the sparse data\n",
    "    data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "    print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "    # Build LDA Model\n",
    "    lda_model = LatentDirichletAllocation(n_components=4,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=1000,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    print(lda_model) \n",
    "    # Define Search Param\n",
    "    print('Grid search starts..')\n",
    "    search_params = {'n_components': [2,3,7,9], 'learning_decay': [.5, .7, .9,.8]}\n",
    "\n",
    "# Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "    model.fit(data_vectorized)\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "   \n",
    "\n",
    "    results = pd.DataFrame(model.cv_results_)\n",
    "\n",
    "    current_palette = sns.color_palette(\"Set2\", 4)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    sns.lineplot(data=results,\n",
    "     x='param_n_components',\n",
    "     y='mean_test_score',\n",
    "     hue='param_learning_decay',\n",
    "     palette=current_palette,\n",
    "   marker='o')\n",
    "    plt.savefig('C:\\\\Users\\\\hp\\\\Desktop\\\\MS PROJECT\\\\Politics LDA\\\\politics_LDA'+ i+'.png', dpi=400)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "\n",
    "# Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "    pyLDAvis.enable_notebook()\n",
    "    panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "\n",
    "    panel\n",
    "    print('Saving')\n",
    "    pyLDAvis.save_html(panel, 'C:\\\\Users\\\\hp\\\\Desktop\\\\MS PROJECT\\\\Politics LDA\\\\politics_LDA'+ i+'.html')\n",
    "    # Topic-Keyword Matrix\n",
    "    df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "    df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "    df_topic_keywords.index = topicnames\n",
    "    df_topic_keywords.to_csv('C:\\\\Users\\\\hp\\\\Desktop\\\\MS PROJECT\\\\Politics LDA\\\\politics_LDA'+ i+'.csv')\n",
    "    \n",
    "\n",
    "    joblib.dump(best_lda_model, 'C:\\\\Users\\\\hp\\\\Desktop\\\\MS PROJECT\\\\Politics LDA\\\\politics_LDA'+ i+'.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testml = joblib.load(\"C:\\\\Users\\\\hp\\\\Desktop\\\\MS PROJECT\\\\Politics LDA\\\\politics_LDA01-03-2020.jl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testml.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['tweet']=dataPreprocessing(papers['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percen\n",
    "\n",
    "00\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|(||(|((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((999tage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=6,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [2, 4, 6, 8, 10], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the k-means clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=15, random_state=100).fit_predict(lda_output)\n",
    "\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "\n",
    "# Weights for the 15 columns of lda_output, for each component\n",
    "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "\n",
    "# Percentage of total information in 'lda_output' explained by the two components\n",
    "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
